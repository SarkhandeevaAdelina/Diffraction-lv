\documentclass[a4paper,12pt]{article}
\usepackage[english, russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{titletoc}
\usepackage{caption}
\usepackage{setspace}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{indentfirst} % обеспечивает красную строку в первом абзаце раздела

\geometry{a4paper, margin=2.5cm}
\setlength{\parindent}{1.25cm} 

\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\contentsname}{Содержание}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}

\lstset{style=mystyle}
\usepackage{caption}
\captionsetup[figure]{labelsep=endash, name={Рисунок}}
\captionsetup[lstlisting]{justification=centering, singlelinecheck=true, labelsep=endash}



\begin{document}
	\begin{titlepage}
		\thispagestyle{empty} % убираем нумерацию с титульного листа
		\begin{center}
			\large{МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ}
			\large{ФЕДЕРАЛЬНОЕ ГОСУДАРСТВЕННОЕ АВТОНОМНОЕ ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ ВЫСШЕГО ОБРАЗОВАНИЯ}
			
			\large{«КАЗАНСКИЙ (ПРИВОЛЖСКИЙ) ФЕДЕРАЛЬНЫЙ УНИВЕРСИТЕТ»}
			
			\large{Институт вычислительной математики и информационных технологий}
			
			\large{Кафедра прикладной математики и искусственного интеллекта}\\[50mm]
			
			\large{\textbf{ОТЧЕТ}}\\
			\large    {по дисциплине «Технологии программирования CUDA»}
			
			\large{Дифракция электромагнитной волны на металлической пластине с учетом скин-слоя}\\[80mm]
			
			\begin{flushright}
				
				Выполнил:
				
				студент группы 09-222
				
				Сархандеева А. А.
				
				Проверил:
				
				к.ф.-м.н., доцент Д.Н.Тумаков
				
			\end{flushright}
			
			\vfill

			{\large Казань, 2025 год}
			
		\end{center}
		
		\pagenumbering{gobble}
		\pagenumbering{arabic}
	\end{titlepage}
	
	\newpage
	\setcounter{page}{1}
	\tableofcontents
	\thispagestyle{empty}
	\pagebreak
	
	
	\newpage
	\section{Постановка задачи}
	Рассмотрим задачу падения электромагнитной волны вида $u_{0}(x,y)=A_{0}e^{ik \sin\phi x + ik \cos \phi y}$ на металлическую пластину, размещенную в плоскости $z=0$ декартовой системы координат параллельно оси $y$, где $\phi$ — угол, отсчитываемый от оси $z$.
	
	\begin{figure}[h]
		
		\begin{picture}(280,150)(-50,-15)
			\put(100,50){\vector(1,0){150}}
			\put(130,10){\vector(0,1){120}}
			\put(250,44){$x$}
			\put(134,127){$z$}
			\multiput(100,120)(45,0){3}{\vector(1,-1){40}}
			\thicklines
			\put(140,49){\line(1,0){31}} \put(140,51){\line(1,0){31}} \put(140,50){\line(1,0){31}}
			\put(130,50){\circle{2}}
			\put(120,38){$0$}
			\put(160,59){$u_1, \ v_1$}
			\put(165,40){$u_2, \ v_2$}
			\put(245,23){$D_2$}
			\put(245,70){$D_1$}
		\end{picture}
		\vspace{-30pt}\caption{Падение электромагнитной волны на металлическую пластину}
		\label{fig:1}
	\end{figure}
	
	Необходимо найти электромагнитное поле, возникающее при её дифракции. Рассмотрим случай, когда вектор $E$ падающей волны также параллелен оси $y$ ($TE$-поляризация поля). Поэтому будем искать решение задачи дифракции, не зависящее от координаты $y$.
	
	Сформулируем следующую теорему:
	
	\textbf{Теорема 1.} Задача дифракции $TE$-поляризованной электромагнитной волны на металлическом экране $\left[a, b\right]$ со скин-слоем сводится к интегральному уравнению Фредгольма второго рода:
	\begin{equation}
		-\chi y(x) + \int_{a}^{b} y (\tau)K(\tau - x)d\tau = 2u_{0}(x,0), \quad x\in \left[a, b\right],
		\label{eq:main_equation}
	\end{equation}
	относительно скачка магнитной напряженности на металлическом экране $y(x)=v_{2}(x)-v_{1}(x)$.
	
	Здесь $u_0(x,0)$ — поле падающей волны, $\chi$ — параметр, обратно пропорциональный глубине скин-слоя $\delta$ и характеризующий поверхностный импеданс экрана, ядро $K(\tau, x)$ уравнения содержит логарифмическую особенность и представляется в виде
	\[
	K(\tau, x) = -\ln\frac{1}{|\tau - x|} + r(\tau, x),
	\]
	где $r(\tau, x)$ — регулярная часть.
	
	Важно отметить, что уравнение (\ref{eq:main_equation}) при малых $\chi$ вырождается.
	
	Для численного решения применяется метод моментов с базисными функциями, построенными на полиномах Чебышева первого рода с весом $\rho(x) = \tfrac{1}{\sqrt{(b - x)(x - a)}}$
	\begin{equation}
		y(x) \approx \rho(x) \sum_{n=1}^{N} a_n \widetilde{T}_{n-1}(x),
		\label{eq:solution_expansion}
	\end{equation}
	где $\widetilde{T}_n(x)$ — полиномы Чебышева.
	
	Применяя метод моментов (Галёркина), получаем систему линейных алгебраических уравнений:
	\begin{equation}
		\sum_{n=1}^{N} a_n \left( \chi \pi \delta_{kn} + \eta_{kn} + r_{kn} \right) = f_k, \quad k=1,\dots,N,
		\label{eq:sla_system}
	\end{equation}
	где $\delta_{kn}$ — символ Кронекера, $\eta_{kn}$ — коэффициенты от сингулярной части ядра, $r_{kn}$ — коэффициенты от регулярной части ядра, $f_k$ — проекции падающего поля на базисные функции.
	
	Для вычисления коэффициентов $r_{kn}$ и $f_k$ используется квадратурная формула Гаусса–Чебышева:
	\begin{equation}
		\int_{a}^{b} \frac{f(t)}{\sqrt{(b-t)(t-a)}} dt \approx \frac{\pi}{M} \sum_{m=1}^{M} f(x_m),
		\label{eq:quadrature}
	\end{equation}
	с узлами
	\[
	x_m = \frac{b-a}{2} \cos\left(\frac{2m-1}{2M}\pi\right) + \frac{b+a}{2}, \quad m = 1,\dots,M.
	\]
	
\newpage
\section{Реализация на CPU}

Для численного решения задачи дифракции разработана последовательная реализация на языке C++, основанная на методе моментов с базисом из полиномов Чебышева. Основной вычислительный цикл включает следующие два этапа.

\begin{enumerate}
	\item Построение матрицы коэффициентов системы линейных уравнений (\ref{eq:sla_system}) и вектора правой части $f_k$. Этот этап реализован в методе \texttt{DifrOnLenta::SolveDifr()} и имеет вычислительную сложность $O(N^2 M^2)$, где $N$ — число базисных функций, а $M$ — число узлов квадратуры Гаусса–Чебышева.
	
	\begin{lstlisting}[caption={Построение матрицы на CPU}]
		for (int k = 0; k < N; k++) {
			for (int j = 0; j < N; j++) {
				Complex s(0.0, 0.0);
				for (int m = 0; m < M; m++) {
					for (int n = 0; n < M; n++) {
						s += r(x_points[n], x_points[m]) *
						ChebAB(j, x_points[n]) *
						ChebAB(k, x_points[m]);
					}
				}
				s *= M_PI * M_PI / (M * M);
				A[k][j] = s;
			}
		}
	\end{lstlisting}
	
	\item Решение СЛАУ с помощью реализованного вручную метода Гаусса с частичным выбором главного элемента.
	
	\begin{lstlisting}[caption={Метод Гаусса на CPU}]
		int Gauss(CMatr& A, CVect& b, CVect& x) {
			int N = b.Size();
			for (int i = 0; i < N - 1; i++) {	// ...
				for (int j = i + 1; j < N; j++) {
					for (int k = i + 1; k < N; k++) {
						A[j][k] = A[j][k] - s1 * A[i][k];
					}
					b[j] = b[j] - b[i] * s1;
				}} // ...}
	\end{lstlisting}
	
	Сложность этого этапа — $O(N^3)$, что делает его вычислительно затратным уже при умеренных значениях $N \sim 200$.
\end{enumerate}

Из-за квадратичной зависимости от $N$ и двойного внутреннего цикла по $M$, время построения матрицы быстро растёт. Аналогично, метод Гаусса, не оптимизированный для современных процессоров, оказывается крайне медленным.

На рисунке~\ref{fig:cpu_performance} представлено распределение времени выполнения программы по операциям в зависимости от параметра усечения $N$. Как видно из графика, основная доля времени ($>99\%$) тратится на построение матрицы, тогда как решение СЛАУ занимает менее $1\%$ времени. Это подтверждает, что именно построение матрицы является наиболее трудозатратным в CPU-реализации.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{timing_analysis.png}
	\captionsetup{justification=centering, singlelinecheck=false}
	\caption{Зависимость времени выполнения и его распределения по операциям от параметра усечения $N$ (CPU)}
	\label{fig:cpu_performance}
\end{figure}

В таблице 1 приведены точные значения времени выполнения для различных значений $N$, которые использовались при построении графиков.

\begin{table}[h]
	\centering
	\hfill Таблица 1\par
	\centering
	Результаты времени выполнения в зависимости от параметра усечения $N$ (CPU) \\
	\vspace{0.5em}
	\begin{tabular}{|c|c|c|c|}
		\hline
		$N$ & Время матрицы (мс) & Время СЛАУ (мс) & Общее (мс) \\
		\hline
		10 & 5.508 & 0.007 & 5.518 \\
		\hline
		20 & 22.579 & 0.049 & 22.632 \\
		\hline
		30 & 53.720 & 0.144 & 53.869 \\
		\hline
		40 & 101.081 & 0.339 & 101.426 \\
		\hline
		50 & 166.601 & 0.640 & 167.262 \\
		\hline
	\end{tabular}
	\setcounter{table}{1} % <-- Устанавливаем номер таблицы в 1
	\label{tab:cpu_times}
\end{table}

Для подтверждения теоретической квадратичной зависимости времени построения матрицы от размера $N$ выполнена аппроксимация экспериментальных данных полиномом второй степени $T(N) = aN^2 + bN + c$ методом наименьших квадратов. На рисунке~\ref{fig:matrix_fit}  аппроксимирующая кривая точно следует экспериментальным точкам.Таким образом вычислительная сложность алгоритма построения матрицы составляет $O(N^2)$, что соответствует теоретическим оценкам при фиксированном числе квадратурных узлов M.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{matrix_fit.png}
	\captionsetup{justification=centering, singlelinecheck=false}
	\caption{Сравнение экспериментального времени построения матрицы с параболической аппроксимацией (CPU)}
	\label{fig:matrix_fit}
\end{figure}
\clearpage
\section{Реализация на CUDA}

Для ускорения вычислений реализована CUDA-версия программы, в которой основные вычислительно-ёмкие этапы — построение матрицы системы (\ref{eq:sla_system}) и её решение — выполняются на GPU. Архитектура GPU идеально подходит для данных задач благодаря высокой степени параллелизма.

\subsection{Построение матрицы коэффициентов на GPU}

Поскольку вычисление каждого элемента матрицы $A_{kj}$ не зависит от других, данный этап допускает полную параллелизацию.

Для реализации на GPU использовано двумерное CUDA-ядро \texttt{BuildMatrixKernel}, в котором каждый поток обрабатывает один элемент матрицы. Индексы вычисляются по стандартной схеме.

\begin{lstlisting}[caption={CUDA-ядро построения матрицы}]
	CUDA_GLOBAL void BuildMatrixKernel(Complex* matrix,
	const double* xPoints, DeviceParams params, int quadPoints) {
		int j = blockIdx.x * blockDim.x + threadIdx.x;
		int k = blockIdx.y * blockDim.y + threadIdx.y;
		if (j >= params.N || k >= params.N) return;
		
		Complex sum(0.0, 0.0);
		for (int m = 0; m < quadPoints; m++) {
			double xm = xPoints[m];
			double Tk = ChebABValue(k, xm, params.a, params.b);
			for (int n = 0; n < quadPoints; n++) {
				double xn = xPoints[n];
				Complex kernel = RKernel(xn, xm, params.lambda);
				double Tj = ChebABValue(j, xn, params.a, params.b);
				sum += kernel * Tj * Tk;
			}
		}
		sum *= M_PI * M_PI / (quadPoints * quadPoints);
		matrix[j * params.N + k] = sum; // column-major
	}
\end{lstlisting}

Ядро запускается на двумерной сетке блоков размером $16 \times 16$.

\begin{lstlisting}[caption={Запуск ядра построения матрицы на двумерной сетке}]
	dim3 block(16, 16);
	dim3 grid((N + block.x - 1) / block.x,
	(N + block.y - 1) / block.y);
	BuildMatrixKernel<<<grid, block>>>(d_matrix, d_xPoints, params, quadPoints);
\end{lstlisting}

Аналогично, вектор правой части строится с помощью одномерного ядра.

\begin{lstlisting}[caption={CUDA-ядро построения вектора правой части}]
	CUDA_GLOBAL void BuildRhsKernel(Complex* rhs, const double* xPoints,
	DeviceParams params, int quadPoints) {
		int k = blockIdx.x * blockDim.x + threadIdx.x;
		if (k >= params.N) return;
		Complex sum(0.0, 0.0);
		for (int m = 0; m < quadPoints; m++) {
			double xm = xPoints[m];
			sum += FKernel(xm, params.lambda, params.theta) *
			ChebABValue(k, xm, params.a, params.b);
		}
		rhs[k] = sum * M_PI / quadPoints;
	}
\end{lstlisting}

Такой подход позволяет загрузить вычислительные ресурсы GPU в полной мере и существенно сократить время построения матрицы по сравнению с последовательной реализацией.

\subsection{Решение СЛАУ на GPU}

Решение системы линейных уравнений в CPU-версии реализовано на основе метода Гаусса с частичным выбором главного элемента.

В GPU-версии использована библиотека \texttt{cuSOLVER}, входящая в состав CUDA Toolkit и предназначенная для решения задач линейной алгебры на GPU. Решение СЛАУ производится в три этапа.

\begin{enumerate}
	\item Определение объёма рабочего буфера.
	\begin{lstlisting}[caption={Вычисление размера рабочего буфера для LU-разложения}]
		cusolverDnZgetrf_bufferSize(cusolverH, N, N,
		reinterpret_cast<cuDoubleComplex*>(d_matrix), N, &workSize);
	\end{lstlisting}
	
	\item LU-разложение матрицы.
	\begin{lstlisting}[caption={LU-разложение матрицы с помощью cuSOLVER}]
		cusolverDnZgetrf(cusolverH, N, N,
		reinterpret_cast<cuDoubleComplex*>(d_matrix), N,
		reinterpret_cast<cuDoubleComplex*>(d_work), d_ipiv, d_info);
	\end{lstlisting}
	
	\item Решение системы с уже разложенной матрицей.
	\begin{lstlisting}[caption={Решение СЛАУ по факторизованной матрице}]
		cusolverDnZgetrs(cusolverH, CUBLAS_OP_N, N, 1,
		reinterpret_cast<cuDoubleComplex*>(d_matrix), N, d_ipiv,
		reinterpret_cast<cuDoubleComplex*>(d_rhs), N, d_info);
	\end{lstlisting}
\end{enumerate}

Все операции выполняются непосредственно в памяти устройства, что исключает необходимость копирования промежуточных данных на хост. Использование \texttt{cuSOLVER} обеспечивает численную устойчивость и высокую производительность без необходимости ручной реализации сложных алгоритмов.

В таблице 2 приведены точные значения времени выполнения для различных значений $N$, которые использовались при построении графиков.
\clearpage
\begin{table}[h]
	\centering
	\hfill Таблица 2\par
	\centering
	Результаты времени выполнения в зависимости от параметра усечения $N$ (GPU) \\
	\vspace{0.5em}
	\begin{tabular}{|c|c|c|c|}
		\hline
		$N$ & Время матрицы (мс) & Время СЛАУ (мс) & Общее (мс) \\
		\hline
		10 & 13.603 & 0.528 & 14.269 \\
		\hline
		20 & 21.794 & 0.545 & 22.478 \\
		\hline
		30 & 21.797 & 0.591 & 22.539 \\
		\hline
		40 & 22.415 & 0.601 & 23.186 \\
		\hline
		50 & 23.080 & 0.635 & 23.894 \\
		\hline
	\end{tabular}
	\setcounter{table}{1} % <-- Устанавливаем номер таблицы в 1
	\label{tab:cpu_times2}
\end{table}

Для GPU-версии также была выполнена аппроксимация экспериментальных данных
полиномом второй степени методом наименьших квадратов. Однако, в отличие от CPU-реализации, полученная
аппроксимирующая кривая на рисунке ~\ref{fig:matrix_fit_gpu} не отражает реальный характер зависимости времени
построения матрицы от параметра усечения~$N$.
Это объясняется тем, что вычисление элементов матрицы полностью
распараллелено, и рост вычислительной сложности компенсируется увеличением
степени параллелизма GPU. В исследуемом диапазоне значений~$N$ время
выполнения в основном определяется накладными расходами запуска CUDA-ядер и
доступом к памяти, а не асимптотической сложностью алгоритма.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{gpu_matrix_fit.png}
	\captionsetup{justification=centering, singlelinecheck=false}
	\caption{Сравнение экспериментального времени построения матрицы с параболической аппроксимацией (GPU)}
	\label{fig:matrix_fit_gpu}
\end{figure}
\clearpage
\subsection{Технические характеристики среды выполнения}

Эксперименты проводились в Google Colab на видеокарте NVIDIA Tesla T4. Хотя GPU является виртуализированным ресурсом среды, его вычислительные возможности достаточны для демонстрации эффективности CUDA-ускорения.
\clearpage
\section{Сравнение производительности CPU и GPU}

Для объективной оценки эффективности выполнения расчётов на GPU проведена серия тестов при $N \in \{10, 20, 50, 100, 200, 500\}$. Результаты представлены на рисунке~\ref{fig:speedup}, где по оси ординат отложено ускорение (отношение времени выполнения на CPU к времени на GPU), а по оси абсцисс — параметр усечения $N$. Шкала по вертикали — логарифмическая, что позволяет наглядно оценить диапазон ускорений. На графике изображены три зависимости: синяя линия соответствует ускорению этапа построения матрицы коэффициентов системы~(\ref{eq:sla_system}), зелёная — решения СЛАУ, красная — общему ускорению всей вычислительной процедуры.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{compare.png}
	\captionsetup{justification=centering, singlelinecheck=false}
	\caption{Эффективность GPU-ускорения в зависимости от числа базисных функций $N$}
	\label{fig:speedup}
\end{figure}

В таблицах 3 и 4 приведены результаты замеров времени выполнения для CPU- и GPU-реализаций соответственно, использованные при построении графика.
\clearpage
\begin{table}[htbp]
\centering
\hfill{Таблица 3}\par
\centering
Время выполнения CPU-реализации (в миллисекундах) для разных значений $N$ \\
\vspace{0.5em}
	\begin{tabular}{|c|c|c|c|}
		\hline
		$N$ & Время матрицы (мс) & Время СЛАУ (мс) & Общее (мс) \\
		\hline
		10 & 5.337 & 0.010 & 5.348 \\
		\hline
		20 & 23.304 & 0.065 & 23.377 \\
		\hline
		50 & 209.193 & 0.818 & 210.032 \\
		\hline
		100 & 1350.769 & 6.439 & 1357.298 \\
		\hline
		200 & 10129.443 & 50.740 & 10180.418 \\
		\hline
		500 & 149312.741 & 1367.957 & 150682.391 \\
		\hline
	\end{tabular}
	\label{tab:cpu_performance}
\end{table}

\begin{table}[htbp]
\centering
\hfill{Таблица 4}\par
\centering
Время выполнения GPU-реализации (в миллисекундах) для разных значений $N$ \\
\vspace{0.5em}
	\begin{tabular}{|c|c|c|c|}
		\hline
		$N$ & GPU матрица (мс) & GPU СЛАУ (мс) & Общее (мс) \\
		\hline
		10 & 33.451 & 0.665 & 34.288 \\
		\hline
		20 & 53.620 & 0.661 & 54.461 \\
		\hline
		50 & 39.530 & 0.714 & 40.425 \\
		\hline
		100 & 54.037 & 0.961 & 55.214 \\
		\hline
		200 & 136.601 & 1.710 & 138.552 \\
		\hline
		500 & 715.481 & 5.996 & 721.933 \\
		\hline
	\end{tabular}
	\label{tab:gpu_performance}
\end{table}

Поскольку в CPU-реализации более 99\% общего времени тратится на построение матрицы, как было показано на рисунке~\ref{fig:cpu_performance}, общее ускорение практически совпадает с ускорением этого этапа. При $N = 500$ общее время сокращается с 150.7~с до 0.722~с, что делает GPU-реализацию более чем в 200 раз быстрее.


\clearpage

	\section{Аналитическая оценка сложности и сопоставление с экспериментом}

В данном разделе приводится детальный анализ вычислительной сложности алгоритма построения матрицы системы линейных уравнений. Анализ разделен на два уровня: оценка сложности вычисления одного матричного элемента (не зависящая от размера системы $N$) и оценка сложности построения всей матрицы. Полученные теоретические оценки затем сопоставляются с экспериментальными данными времени выполнения на CPU и GPU.

\subsection{Сложность вычисления одного матричного элемента}

Каждый элемент матрицы $A_{kj}$ представляет собой двойной интеграл, вычисляемый численно с использованием квадратурной формулы Гаусса–Чебышева порядка $M$
\begin{equation}
	A_{kj} \approx \frac{\pi^2}{M^2} \sum_{m=1}^{M} \sum_{n=1}^{M} K(x_m, x_n) T_k(x_m) T_j(x_n),
	\label{eq:matrix_element}
\end{equation}
где $K(x_m, x_n)$ — ядро интегрального уравнения (включая сингулярную и регулярную части), $T_k(x)$ — полиномы Чебышева.

Важным требованием корректной оценки сложности является независимость стоимости вычисления одного элемента от параметра усечения $N$. Это достигается за счет того, что значения базисных функций $T_k(x_m)$ в узлах квадратуры либо вычисляются на лету за константное время (используя тригонометрическое определение $T_k(\cos \theta) = \cos(k\theta)$), либо предварительно табулируются. В рамках данной модели мы полагаем, что вклад вычисления полиномов $T_k$ уже включен в общую стоимость или пренебрежимо мал по сравнению с вычислением ядра $K(x_m, x_n)$.

Основную вычислительную нагрузку несет вычисление ядра $K(x_m, x_n)$, которое включает функции Бесселя $J_0$ и $Y_0$, а также логарифмическую особенность. Рассмотрим составляющие этой сложности в терминах операций с плавающей точкой (FLOPS).

	\subsubsection*{Оценка стоимости функций Бесселя}
Согласно проведенным инструментальным измерениям функции вычисления рядов Бесселя (см. код метода \texttt{Bessel}), была собрана статистика на выборке из $10^6$ точек в диапазоне $x \in [0, 50]$. Сходимость ряда определялась достижением точности $\epsilon = 10^{-4}$ (параметр \texttt{eps} в коде). Результаты статистического анализа приведены в таблице~\ref{tab:bessel_stats}.

\begin{table}[h]
    \centering
    \caption{Статистические характеристики сходимости рядов Бесселя ($\epsilon = 10^{-4}$)}
    \label{tab:bessel_stats}
    \begin{tabular}{|l|c|c|c|}
        \hline
        Функция & Ср. кол-во членов $\langle K \rangle$ & Дисперсия $\sigma^2$ & Ср. квадр. откл. $\sigma$ \\ \hline
        $J_0(x)$ & 10.95 & 2.84 & 1.68 \\ \hline
        $Y_0(x)$ & 11.29 & 3.15 & 1.77 \\ \hline
    \end{tabular}
\end{table}

Малая величина дисперсии подтверждает высокую стабильность алгоритма: количество необходимых членов ряда слабо зависит от значения аргумента в рабочем диапазоне. Это позволяет использовать среднее значение $\langle K \rangle$ как надежную константу для оценки сложности.

\textbf{Пример процесса сходимости}: Для типичного значения аргумента $x=5.0$ расчет функции $J_0$ требует 11 членов ряда. Значения текущего слагаемого $s_k$ убывают следующим образом:
$s_0 = 1.0$, $s_1 \approx -6.25$, $s_2 \approx 9.76$, \dots, $s_{10} \approx 4.3 \times 10^{-5} < \epsilon$.
На каждом таком шаге выполняется фиксированный набор операций. Анализ тела цикла показывает следующую стоимость вычисления одного члена ряда (построчный расчет):
\begin{itemize}
    \item \textbf{Функция $J_0$}: 
        \begin{enumerate}
            \item \texttt{std::abs(s) > eps} --- 1 оп. (модуль и сравнение),
            \item \texttt{sum += s} --- 1 оп. (сложение),
            \item \texttt{k++} --- 1 оп. (инкремент),
            \item \texttt{\_1 = -\_1} --- 1 оп. (смена знака),
            \item \texttt{k * k} --- 1 оп. (умножение),
            \item \texttt{k2 /= (k * k)} --- 1 оп. (деление),
            \item \texttt{xS *= x2} --- 1 оп. (умножение),
            \item \texttt{s = \_1 * k2 * xS} --- 2 оп. (два умножения).
        \end{enumerate}
        Итого: $\approx 9$ FLOPS за шаг цикла.
    \item \textbf{Функция $Y_0$}: 
        Включает все операции $J_0$ плюс расчет гармонической добавки:
        \begin{enumerate}
            \item \texttt{1.0 / k} --- 1 оп. (деление),
            \item \texttt{psi += \dots} --- 1 оп. (сложение),
            \item \texttt{s = \dots * psi} --- 1 оп. (дополнительное умножение).
        \end{enumerate}
        Итого: $9 + 3 = 12$ FLOPS за шаг цикла.
\end{itemize}

Суммарная арифметическая стоимость вычисления рядов:
\[
C_{J_0}^{\text{arith}} = \langle K_{J_0} \rangle \times 9 \approx 99\ \mathrm{FLOPS}, \qquad
C_{Y_0}^{\text{arith}} = \langle K_{Y_0} \rangle \times 12 \approx 135\ \mathrm{FLOPS}.
\]

Дополнительно учитывается вычисление натурального логарифма. Математически функция Бесселя первого рода $J_0$ не содержит логарифмических операций ($C_{\log} = 0$), в то время как функция второго рода $Y_0$ (Неймана) требует однократного вычисления $\ln(x)$ в каждой точке. Измеренное время выполнения $\ln(x)$ на тестовом процессоре составляет $t_{\log} \approx 9.2 \text{ нс}$. При производительности ядра $\approx 17.6$ GFLOP/s это эквивалентно
\[
C_{\log} \approx 9.2 \times 10^{-9} \text{ с} \times 17.6 \times 10^{9} \text{ FLOP/s} \approx 162\ \mathrm{FLOPS}.
\]

Таким образом, полная стоимость вычисления одной пары значений функций Бесселя, составляющей основу ядра,
\[
C_{\text{Bessel}} = (C_{J_0}^{\text{arith}} + 0) + (C_{Y_0}^{\text{arith}} + C_{\log}) \approx 99 + (135 + 162) \approx 396\ \mathrm{FLOPS}.
\]

С учетом операций управления циклом и формирования комплексного ядра в \texttt{RKernel}, примем расчетное значение
\[
C_K \approx 410\ \mathrm{FLOPS}.
\]

Это значение представляет собой константную стоимость вычисления ядра в одной точке, не зависящую ни от $N$, ни от индексов $k, j$.

\subsubsection*{Стоимость формирования элемента матрицы}
Внутри двойного цикла по $m$ и $n$ (см. формулу \ref{eq:matrix_element}) происходит:

1. Вычисление ядра $K$ (стоимость $C_K$).

2. Умножение на значения полиномов $T_k$ и $T_j$. В комплексной арифметике это требует 2 комплексных умножения вещественного числа на комплексное (или наоборот) и одно комплексное сложение. Стоимость комплексного умножения/сложения примем за
\[
C_{\text{ops}} \approx 6\ \mathrm{FLOPS}.
\]

Тогда стоимость одного шага во внутреннем цикле суммирования равна $C_K + +C_{\text{ops}} \approx 416$ FLOPS. Поскольку суммирование ведется по $M \times M$ точкам, стоимость вычисления одного матричного элемента $A_{kj}$ составляет
\[
\mathrm{Cost}_{\text{element}} = M^2 \times (C_K + C_{\text{ops}}).
\]
При значении $M=20$
\[
\mathrm{Cost}_{\text{element}} = 400 \times (410 + 6) \approx 166\,400\ \mathrm{FLOPS}.
\]

	\subsection{Сложность построения всей матрицы}

Матрица системы имеет размер $N \times N$. Следовательно, полная вычислительная сложность этапа построения матрицы определяется как произведение количества элементов на стоимость одного элемента. С учетом того, что каждый элемент требует $M^2$ вычислений ядра, запишем
\begin{equation}
	\mathrm{Ops}_{\text{total}}(N, M) = N^2 \cdot M^2 \cdot (C_K + C_{\text{ops}}).
	\label{eq:total_complexity}
\end{equation}
Подставляя полученное значение $C_K + C_{\text{ops}} \approx 416$ FLOPS. Тогда
\[
\mathrm{Ops}_{\text{total}}(N, M) \approx 416 \cdot N^2 \cdot M^2\ \mathrm{FLOPS}.
\]

Формула наглядно демонстрирует, что сложность зависит квадратично как от размерности базиса $N$, так и от количества узлов квадратуры $M$. При значении $M=20$ получаем оценку $166\,400 \cdot N^2$.


	\subsection{Сравнение с экспериментом и выводы}

Проведем верификацию полученной модели на примере задачи с $N=500$.

1. Теоретическая оценка объема вычислений.
\[
\mathrm{Ops}_{\text{total}}(500) \approx 166\,400 \cdot 500^2 = 166\,400 \cdot 250\,000 \approx 4.16 \times 10^{10}\ \mathrm{FLOPS}.
\]

2. Анализ производительности CPU.
Из таблицы 2 время выполнения на CPU для $N=500$ составляет $t_{\text{CPU}} \approx 149.31$ с.
Фактическая производительность CPU
\[
P_{\text{CPU}} = \frac{4.16 \times 10^{10}}{149.31} \approx 2.78 \times 10^8\ \mathrm{FLOP/s} \approx 0.28\ \mathrm{GFLOP/s}.
\]

Результат является реалистичным для однопоточного выполнения сложной математики (функции Бесселя, логарифмы) на универсальном процессоре без агрессивной векторизации.

3. Анализ производительности GPU.
Из таблицы 3 время выполнения на GPU для $N=500$ составляет $t_{\text{GPU}} \approx 0.715$ с.
Фактическая производительность GPU
\[
P_{\text{GPU}} = \frac{4.16 \times 10^{10}}{0.715} \approx 5.82 \times 10^{10}\ \mathrm{FLOP/s} \approx 58.2\ \mathrm{GFLOP/s}.
\]
Полученная производительность на два порядка выше, чем у CPU, что объясняется массовым параллелизмом GPU.

4. Итоговое ускорение. Отношение производительностей
\[
S = \frac{P_{\text{GPU}}}{P_{\text{CPU}}} \approx \frac{58.2}{0.28} \approx 208.
\]

	Это значение практически идеально совпадает с прямым отношением времен выполнения ($149.31 / 0.715 \approx 209$), что подтверждает корректность уточненной модели оценки сложности.

\subsubsection*{Анализ порогов эффективности (N, M)}
Эффективность перехода на GPU определяется общим объёмом вычислений ядра $K(x, \tau)$, который пропорционален произведению $N^2 \cdot M^2$. Экспериментальные данные позволяют выделить следующую зависимость:
\begin{itemize}
    \item \textbf{Точка пересечения (Crossover)}: GPU становится быстрее CPU при условии $N \cdot M \gtrsim 220$. Например, при фиксированном $M=20$ это соответствует $N \approx 11$. При меньших значениях накладные расходы на инициализацию CUDA-контекста и передачу данных в видеопамять (суммарно $\approx 60$ мс) превышают время вычислений на CPU.
    \item \textbf{Влияние точности $M$}: Рост числа узлов квадратуры $M$ делает использование GPU выгодным даже на малых матрицах $N$. Таким образом, чем выше требования к точности интеграла, тем раньше наступает преимущество параллельной архитектуры.
    \item \textbf{Целевое ускорение (100x)}: Ускорение в 100 раз достигается при $N \cdot M \gtrsim 6000$ (например, $N \approx 300$ при $M=20$).
\end{itemize}

\subsection{Сравнение с учетом тактовых частот}

Для глубокого анализа эффективности архитектур воспользуемся классической формулой производительности системы:
\begin{equation}
    P = f \cdot \text{OPC}_{\text{sys}},
    \label{eq:performance_formula}
\end{equation}
где $P$ — производительность в GFLOP/s, $f$ — тактовая частота в ГГц, а $\text{OPC}_{\text{sys}}$ — суммарное количество операций, выполняемых всей системой за один такт. 

Используя данные измерений, определим реальную вычислительную отдачу за такт:
\begin{itemize}
    \item \textbf{CPU} (2.20 ГГц): $\text{OPC}_{\text{CPU}} = 0.28 / 2.20 \approx 0.127$ FLOP за такт.
    \item \textbf{GPU} (1.59 ГГц Boost): $\text{OPC}_{\text{GPU}} = 58.2 / 1.59 \approx 36.6$ FLOP за такт.
\end{itemize}

Сравнительный анализ показывает, что несмотря на то, что тактовая частота CPU в 1.38 раза выше базовой частоты GPU, за один такт графический ускоритель выполняет почти в 288 раз больше полезной работы. Это наглядно демонстрирует преимущество массово-параллельной архитектуры: вместо попыток наращивания частоты, GPU использует 2560 ядер Tesla T4 для одновременной обработки данных. Таким образом, $P_{\text{GPU}}$ многократно превосходит $P_{\text{CPU}}$ именно за счет множителя $\text{OPC}_{\text{sys}}$.

Таким образом, разделение анализа на "стоимость элемента" (константа) и "стоимость матрицы" ($N^2$) позволило точно описать поведение алгоритма и объяснить наблюдаемый высокий коэффициент ускорения при переходе на графические ускорители.

\clearpage

\section{Выводы}
Выполненный аналитический разбор вычислительной сложности,
подтверждённый экспериментальными измерениями на CPU и GPU,
показывает, что наблюдаемое ускорение при построении матриц
полностью согласуется с теоретической моделью вычислительных затрат.
GPU-реализация демонстрирует более чем 200-кратное ускорение
при больших значениях параметра усечения $N$,
что подтверждено как расчётами, так и фактическими измерениями
времени выполнения.

Дополнительный анализ архитектурных характеристик вычислительных
устройств показал, что полученное ускорение обусловлено
не увеличением тактовой частоты, а массовым параллелизмом GPU,
когда вычисление элементов матрицы распределяется
между тысячами одновременно работающих вычислительных ядер.
Оценка времени вычисления одного элемента матрицы и его сопоставление
с числом арифметических операций и тактовой частотой
дают согласованную количественную картину производительности.


Такое повышение производительности делает возможным решение задач с высокой
точностью, использование больших базисов и выполнение серийных вычислительных тестов
в интерактивном режиме, что существенно расширяет практическую применимость метода.

\end{document}